===================================
ARIA COMPILER SOURCE - PART 8 of 8
Backend: Advanced Features & Runtime Headers
===================================

--- src/backend/codegen_tbb.h ---
#ifndef ARIA_CODEGEN_TBB_H
#define ARIA_CODEGEN_TBB_H

#include <llvm/IR/Value.h>
#include <llvm/IR/Type.h>
#include <llvm/IR/LLVMContext.h>
#include <llvm/IR/Module.h>
#include <llvm/IR/IRBuilder.h>
#include <string>

namespace aria {
namespace backend {

/**
 * TBBLowerer - Twisted Balanced Binary Type Safety Implementation
 * 
 * Implements sticky error propagation for TBB types (tbb8, tbb16, tbb32, tbb64).
 * 
 * CRITICAL REQUIREMENT:
 * The minimum signed value serves as the ERR sentinel:
 *   tbb8:  -128 (0x80)
 *   tbb16: -32768 (0x8000)
 *   tbb32: -2147483648 (0x80000000)
 *   tbb64: -9223372036854775808 (0x8000000000000000)
 * 
 * STICKY ERROR SEMANTICS:
 *   ERR + x = ERR
 *   x + ERR = ERR
 *   overflow(op) = ERR
 *   ERR cannot heal via wrapping
 * 
 * This class intercepts all arithmetic operations on TBB types and injects
 * LLVM intrinsics for overflow detection and sentinel checking.
 */
class TBBLowerer {
    llvm::LLVMContext& llvmContext;
    llvm::IRBuilder<>& builder;
    llvm::Module* module;

public:
    explicit TBBLowerer(llvm::LLVMContext& ctx, llvm::IRBuilder<>& bld, llvm::Module* mod)
        : llvmContext(ctx), builder(bld), module(mod) {}

    /**
     * Check if a type name represents a TBB type.
     * @param typeName The Aria type name (e.g., "tbb8", "int8", "float64")
     * @return true if typeName is tbb8, tbb16, tbb32, or tbb64
     */
    static bool isTBBType(const std::string& typeName);

    /**
     * Get the ERR sentinel value for a given LLVM integer type.
     * Returns the minimum signed value for the bit width.
     * @param type LLVM integer type (i8, i16, i32, i64)
     * @return LLVM constant representing the sentinel value
     */
    llvm::Value* getSentinel(llvm::Type* type);

    /**
     * Create a safe addition operation with sticky error propagation.
     * Equivalent to: (lhs == ERR || rhs == ERR || overflow) ? ERR : lhs + rhs
     * 
     * Uses llvm.sadd.with.overflow intrinsic.
     * 
     * @param lhs Left operand (must be TBB type)
     * @param rhs Right operand (must be TBB type)
     * @return LLVM value representing the safe result
     */
    llvm::Value* createAdd(llvm::Value* lhs, llvm::Value* rhs);

    /**
     * Create a safe subtraction operation with sticky error propagation.
     * Uses llvm.ssub.with.overflow intrinsic.
     */
    llvm::Value* createSub(llvm::Value* lhs, llvm::Value* rhs);

    /**
     * Create a safe multiplication operation with sticky error propagation.
     * Uses llvm.smul.with.overflow intrinsic.
     */
    llvm::Value* createMul(llvm::Value* lhs, llvm::Value* rhs);

    /**
     * Create a safe division operation with sticky error propagation.
     * 
     * Division has special edge cases:
     *   1. Division by zero → ERR
     *   2. ERR / -1 → ERR (would overflow to MAX+1)
     *   3. ERR / x → ERR (input sticky)
     *   4. x / ERR → ERR (input sticky)
     * 
     * @param lhs Dividend (must be TBB type)
     * @param rhs Divisor (must be TBB type)
     * @return LLVM value representing the safe result
     */
    llvm::Value* createDiv(llvm::Value* lhs, llvm::Value* rhs);

    /**
     * Create a safe modulo operation with sticky error propagation.
     * Similar to division, but uses srem instruction.
     */
    llvm::Value* createMod(llvm::Value* lhs, llvm::Value* rhs);

    /**
     * Create a safe negation operation.
     * Special case: -ERR = ERR (negating sentinel stays sentinel)
     * Note: -(MAX+1) would overflow, but MAX+1 is the sentinel, so it's already ERR.
     */
    llvm::Value* createNeg(llvm::Value* operand);

private:
    /**
     * Internal helper for arithmetic operations.
     * @param opCode 0=Add, 1=Sub, 2=Mul
     * @param lhs Left operand
     * @param rhs Right operand
     * @return LLVM value with sticky error propagation
     */
    llvm::Value* createOp(unsigned opCode, llvm::Value* lhs, llvm::Value* rhs);
};

} // namespace backend
} // namespace aria

#endif // ARIA_CODEGEN_TBB_H



--- src/backend/codegen_tbb.cpp ---
#include "codegen_tbb.h"
#include <llvm/IR/Intrinsics.h>
#include <llvm/IR/Constants.h>
#include <llvm/IR/IRBuilder.h>

namespace aria {
namespace backend {

using namespace llvm;

bool TBBLowerer::isTBBType(const std::string& typeName) {
    return typeName == "tbb8" || 
           typeName == "tbb16" || 
           typeName == "tbb32" || 
           typeName == "tbb64";
}

Value* TBBLowerer::getSentinel(Type* type) {
    if (!type->isIntegerTy()) {
        return nullptr;
    }
    
    unsigned width = type->getIntegerBitWidth();
    // Sentinel = minimum signed value for this bit width
    // For i8: -128 (0x80)
    // For i16: -32768 (0x8000)
    // For i32: -2147483648 (0x80000000)
    // For i64: -9223372036854775808 (0x8000000000000000)
    return ConstantInt::get(llvmContext, APInt::getSignedMinValue(width));
}

Value* TBBLowerer::createAdd(Value* lhs, Value* rhs) {
    return createOp(0, lhs, rhs);
}

Value* TBBLowerer::createSub(Value* lhs, Value* rhs) {
    return createOp(1, lhs, rhs);
}

Value* TBBLowerer::createMul(Value* lhs, Value* rhs) {
    return createOp(2, lhs, rhs);
}

Value* TBBLowerer::createOp(unsigned opCode, Value* lhs, Value* rhs) {
    Type* type = lhs->getType();
    Value* sentinel = getSentinel(type);

    // STEP 1: Sticky Input Check
    // If either input is ERR, result must be ERR (no computation needed)
    Value* lhsIsErr = builder.CreateICmpEQ(lhs, sentinel, "lhs_is_err");
    Value* rhsIsErr = builder.CreateICmpEQ(rhs, sentinel, "rhs_is_err");
    Value* inputErr = builder.CreateOr(lhsIsErr, rhsIsErr, "input_err");

    // STEP 2: Perform Operation with Overflow Detection
    Value* rawResult = nullptr;
    Value* overflow = nullptr;
    Intrinsic::ID intrinsicID;

    switch (opCode) {
        case 0: intrinsicID = Intrinsic::sadd_with_overflow; break;
        case 1: intrinsicID = Intrinsic::ssub_with_overflow; break;
        case 2: intrinsicID = Intrinsic::smul_with_overflow; break;
        default: 
            // Fallback: return sentinel for unknown operation
            return sentinel;
    }

    // Get the LLVM intrinsic function
    Function* intrinsic = Intrinsic::getDeclaration(
        module, 
        intrinsicID, 
        {type}
    );

    // Call intrinsic: returns {result, overflow_flag}
    Value* resultStruct = builder.CreateCall(intrinsic, {lhs, rhs});
    rawResult = builder.CreateExtractValue(resultStruct, 0, "raw_result");
    overflow = builder.CreateExtractValue(resultStruct, 1, "overflow");

    // STEP 3: Result Sentinel Check
    // Even if overflow didn't occur, if the calculated result happens to equal
    // the sentinel bit pattern, it represents ERR in TBB semantics.
    // Example: For tbb8, if 100 + 28 = 128, but 128 doesn't fit in signed i8,
    // the bit pattern is 0x80 which is -128 (the sentinel).
    Value* resultIsSentinel = builder.CreateICmpEQ(
        rawResult, 
        sentinel, 
        "result_is_sentinel"
    );

    // STEP 4: Combine All Error Conditions
    // Result is ERR if:
    //   - Either input was ERR (sticky)
    //   - Overflow occurred
    //   - Result bit pattern equals sentinel
    Value* anyError = builder.CreateOr(inputErr, overflow, "has_overflow");
    anyError = builder.CreateOr(anyError, resultIsSentinel, "any_error");

    // STEP 5: Select Final Result
    // If any error condition is true, return sentinel; otherwise return raw result
    return builder.CreateSelect(
        anyError, 
        sentinel, 
        rawResult, 
        "tbb_result"
    );
}

Value* TBBLowerer::createDiv(Value* lhs, Value* rhs) {
    Type* type = lhs->getType();
    Value* sentinel = getSentinel(type);

    // STEP 1: Check Input Sticky Errors
    Value* lhsIsErr = builder.CreateICmpEQ(lhs, sentinel, "lhs_is_err");
    Value* rhsIsErr = builder.CreateICmpEQ(rhs, sentinel, "rhs_is_err");
    Value* inputErr = builder.CreateOr(lhsIsErr, rhsIsErr, "input_err");

    // STEP 2: Check Division by Zero
    Value* zero = ConstantInt::get(type, 0);
    Value* divByZero = builder.CreateICmpEQ(rhs, zero, "div_by_zero");

    // STEP 3: Check Overflow Case (ERR / -1)
    // The only signed division overflow is: MIN_INT / -1 = MAX_INT + 1
    // For TBB, MIN_INT is the sentinel, so ERR / -1 must remain ERR
    Value* minusOne = ConstantInt::get(type, -1, true);
    Value* rhsIsMinusOne = builder.CreateICmpEQ(rhs, minusOne, "rhs_is_minus_one");
    Value* lhsIsSentinel = builder.CreateICmpEQ(lhs, sentinel, "lhs_is_sentinel");
    Value* overflowCase = builder.CreateAnd(
        lhsIsSentinel, 
        rhsIsMinusOne, 
        "overflow_case"
    );

    // STEP 4: Perform Safe Division
    // To avoid CPU traps, use a safe divisor (1) when error is detected
    Value* hasUnsafeDiv = builder.CreateOr(divByZero, overflowCase, "unsafe_div");
    Value* safeDivisor = builder.CreateSelect(
        hasUnsafeDiv, 
        ConstantInt::get(type, 1), 
        rhs, 
        "safe_divisor"
    );

    // Perform the actual division with the safe divisor
    Value* rawResult = builder.CreateSDiv(lhs, safeDivisor, "raw_div");

    // STEP 5: Check if Result is Sentinel (edge case coverage)
    Value* resultIsSentinel = builder.CreateICmpEQ(
        rawResult, 
        sentinel, 
        "result_is_sentinel"
    );

    // STEP 6: Combine All Error Conditions
    Value* totalError = builder.CreateOr(inputErr, hasUnsafeDiv, "has_error");
    totalError = builder.CreateOr(totalError, resultIsSentinel, "total_error");

    // STEP 7: Select Final Result
    return builder.CreateSelect(
        totalError, 
        sentinel, 
        rawResult, 
        "tbb_div"
    );
}

Value* TBBLowerer::createMod(Value* lhs, Value* rhs) {
    Type* type = lhs->getType();
    Value* sentinel = getSentinel(type);

    // STEP 1: Sticky Input Check
    Value* lhsIsErr = builder.CreateICmpEQ(lhs, sentinel, "lhs_is_err");
    Value* rhsIsErr = builder.CreateICmpEQ(rhs, sentinel, "rhs_is_err");
    Value* inputErr = builder.CreateOr(lhsIsErr, rhsIsErr, "input_err");

    // STEP 2: Check Modulo by Zero
    Value* zero = ConstantInt::get(type, 0);
    Value* modByZero = builder.CreateICmpEQ(rhs, zero, "mod_by_zero");

    // STEP 3: Check MIN % -1 Overflow
    // On x86-64, INT_MIN % -1 causes a hardware exception (SIGFPE)
    // Must be caught and handled explicitly to propagate ERR
    Value* minusOne = ConstantInt::get(type, -1, true);
    Value* lhsIsMin = builder.CreateICmpEQ(lhs, sentinel, "lhs_is_min");
    Value* rhsIsMinusOne = builder.CreateICmpEQ(rhs, minusOne, "rhs_is_minus_one");
    Value* minModMinusOne = builder.CreateAnd(lhsIsMin, rhsIsMinusOne, "min_mod_minus_one");

    // STEP 4: Safe Computation Path
    // If mod-by-zero or MIN % -1, use safe divisor (1) to avoid CPU trap
    Value* hasUnsafeMod = builder.CreateOr(modByZero, minModMinusOne, "unsafe_mod");
    Value* safeDivisor = builder.CreateSelect(
        hasUnsafeMod,
        ConstantInt::get(type, 1),
        rhs,
        "safe_divisor"
    );

    // Perform modulo with safe divisor
    Value* rawResult = builder.CreateSRem(lhs, safeDivisor, "raw_mod");

    // STEP 5: Sentinel Collision Detection
    // Even if no overflow occurred, if rawResult == sentinel bit pattern,
    // it's ambiguous with ERR. Must coerce to ERR to maintain sticky semantics.
    Value* resultIsSentinel = builder.CreateICmpEQ(rawResult, sentinel, "result_is_sentinel");

    // STEP 6: Combine All Error Conditions
    Value* totalError = builder.CreateOr(inputErr, hasUnsafeMod, "has_error");
    totalError = builder.CreateOr(totalError, resultIsSentinel, "total_error");

    // STEP 7: Select Final Result
    return builder.CreateSelect(totalError, sentinel, rawResult, "tbb_mod");
}

Value* TBBLowerer::createNeg(Value* operand) {
    Type* type = operand->getType();
    Value* sentinel = getSentinel(type);

    // Check if input is already ERR
    Value* inputIsErr = builder.CreateICmpEQ(operand, sentinel, "input_is_err");

    // Perform negation: -x
    Value* zero = ConstantInt::get(type, 0);
    Value* rawResult = builder.CreateSub(zero, operand, "raw_neg");

    // Check for overflow: -MIN_INT overflows to MIN_INT (the sentinel)
    // Use intrinsic for safety
    Function* intrinsic = Intrinsic::getDeclaration(
        module,
        Intrinsic::ssub_with_overflow,
        {type}
    );
    Value* resultStruct = builder.CreateCall(intrinsic, {zero, operand});
    Value* overflow = builder.CreateExtractValue(resultStruct, 1);

    // Check if result is sentinel
    Value* resultIsSentinel = builder.CreateICmpEQ(rawResult, sentinel);

    // Combine error conditions
    Value* anyError = builder.CreateOr(inputIsErr, overflow);
    anyError = builder.CreateOr(anyError, resultIsSentinel);

    return builder.CreateSelect(anyError, sentinel, rawResult, "tbb_neg");
}

} // namespace backend
} // namespace aria



--- src/backend/monomorphization.h ---
/**
 * src/backend/monomorphization.h
 *
 * Monomorphization Engine for Trait Static Dispatch
 * 
 * Clones trait method implementations and specializes them for concrete types.
 * Generates specialized function names with type mangling.
 */

#ifndef ARIA_BACKEND_MONOMORPHIZATION_H
#define ARIA_BACKEND_MONOMORPHIZATION_H

#include "../frontend/ast.h"
#include "../frontend/ast/stmt.h"
#include <memory>
#include <string>
#include <map>
#include <vector>

namespace aria {
namespace backend {

// Monomorphization context - tracks specializations
struct MonomorphizationContext {
    // Map from (trait_name, type_name, method_name) -> specialized_function_name
    std::map<std::tuple<std::string, std::string, std::string>, std::string> specialization_map;
    
    // Cache of generated specialized functions
    std::vector<std::unique_ptr<frontend::FuncDecl>> specialized_functions;
    
    // Map from trait name to trait declaration
    std::map<std::string, frontend::TraitDecl*> trait_table;
    
    // Map from trait name to implementations
    std::multimap<std::string, frontend::ImplDecl*> impl_table;
};

// Monomorphization engine
class Monomorphizer {
private:
    MonomorphizationContext& context;
    
    // Generate specialized function name
    // Format: {trait}_{type}_{method}
    std::string generateSpecializedName(
        const std::string& trait_name,
        const std::string& type_name,
        const std::string& method_name
    );
    
    // Clone a function declaration for specialization
    std::unique_ptr<frontend::FuncDecl> cloneFuncDecl(frontend::FuncDecl* original);
    
    // Clone an expression (deep copy)
    std::unique_ptr<frontend::Expression> cloneExpr(frontend::Expression* expr);
    
    // Clone a statement (deep copy)
    std::unique_ptr<frontend::Statement> cloneStmt(frontend::Statement* stmt);
    
    // Clone a block (deep copy)
    std::unique_ptr<frontend::Block> cloneBlock(frontend::Block* block);
    
public:
    Monomorphizer(MonomorphizationContext& ctx) : context(ctx) {}
    
    // Register a trait declaration
    void registerTrait(frontend::TraitDecl* trait) {
        context.trait_table[trait->name] = trait;
    }
    
    // Register a trait implementation
    void registerImpl(frontend::ImplDecl* impl) {
        context.impl_table.insert({impl->trait_name, impl});
    }
    
    // Get or create specialized function for a trait method call
    // Returns the specialized function name
    std::string getOrCreateSpecialization(
        const std::string& trait_name,
        const std::string& type_name,
        const std::string& method_name
    );
    
    // Monomorphize all registered trait implementations
    // Returns vector of specialized function declarations
    std::vector<frontend::FuncDecl*> monomorphizeAll();
};

} // namespace backend
} // namespace aria

#endif // ARIA_BACKEND_MONOMORPHIZATION_H



--- src/backend/monomorphization.cpp ---
/**
 * src/backend/monomorphization.cpp
 *
 * Monomorphization Engine Implementation
 */

#include "monomorphization.h"
#include "../frontend/ast/expr.h"
#include "../frontend/ast/control_flow.h"
#include "../frontend/ast/loops.h"
#include "../frontend/ast/defer.h"
#include <sstream>
#include <stdexcept>

namespace aria {
namespace backend {

// Generate specialized function name with type mangling
std::string Monomorphizer::generateSpecializedName(
    const std::string& trait_name,
    const std::string& type_name,
    const std::string& method_name
) {
    std::stringstream ss;
    ss << trait_name << "_" << type_name << "_" << method_name;
    return ss.str();
}

// Clone expression (deep copy for AST specialization)
std::unique_ptr<frontend::Expression> Monomorphizer::cloneExpr(frontend::Expression* expr) {
    if (!expr) return nullptr;
    
    // Use dynamic_cast to determine expression type and clone appropriately
    
    // Literals
    if (auto* intLit = dynamic_cast<frontend::IntLiteral*>(expr)) {
        return std::make_unique<frontend::IntLiteral>(intLit->value);
    }
    
    if (auto* floatLit = dynamic_cast<frontend::FloatLiteral*>(expr)) {
        return std::make_unique<frontend::FloatLiteral>(floatLit->value);
    }
    
    if (auto* boolLit = dynamic_cast<frontend::BoolLiteral*>(expr)) {
        return std::make_unique<frontend::BoolLiteral>(boolLit->value);
    }
    
    if (auto* strLit = dynamic_cast<frontend::StringLiteral*>(expr)) {
        return std::make_unique<frontend::StringLiteral>(strLit->value);
    }
    
    if (auto* nullLit = dynamic_cast<frontend::NullLiteral*>(expr)) {
        return std::make_unique<frontend::NullLiteral>();
    }
    
    // Variables
    if (auto* varExpr = dynamic_cast<frontend::VarExpr*>(expr)) {
        return std::make_unique<frontend::VarExpr>(varExpr->name);
    }
    
    // Binary operations
    if (auto* binOp = dynamic_cast<frontend::BinaryOp*>(expr)) {
        return std::make_unique<frontend::BinaryOp>(
            binOp->op,
            cloneExpr(binOp->left.get()),
            cloneExpr(binOp->right.get())
        );
    }
    
    // Unary operations
    if (auto* unOp = dynamic_cast<frontend::UnaryOp*>(expr)) {
        return std::make_unique<frontend::UnaryOp>(
            unOp->op,
            cloneExpr(unOp->operand.get())
        );
    }
    
    // Function calls
    if (auto* callExpr = dynamic_cast<frontend::CallExpr*>(expr)) {
        std::unique_ptr<frontend::CallExpr> cloned;
        if (!callExpr->function_name.empty()) {
            cloned = std::make_unique<frontend::CallExpr>(callExpr->function_name);
        } else if (callExpr->callee) {
            cloned = std::make_unique<frontend::CallExpr>(cloneExpr(callExpr->callee.get()));
        } else {
            cloned = std::make_unique<frontend::CallExpr>("");
        }
        for (const auto& arg : callExpr->arguments) {
            cloned->arguments.push_back(cloneExpr(arg.get()));
        }
        cloned->type_arguments = callExpr->type_arguments;
        return cloned;
    }
    
    // For other expression types, return nullptr
    // This is safe because monomorphization only needs to clone function bodies
    // which typically contain basic expressions
    return nullptr;
}

// Clone statement (deep copy)
std::unique_ptr<frontend::Statement> Monomorphizer::cloneStmt(frontend::Statement* stmt) {
    if (!stmt) return nullptr;
    
    // Return statement
    if (auto* retStmt = dynamic_cast<frontend::ReturnStmt*>(stmt)) {
        return std::make_unique<frontend::ReturnStmt>(cloneExpr(retStmt->value.get()));
    }
    
    // Variable declaration
    if (auto* varDecl = dynamic_cast<frontend::VarDecl*>(stmt)) {
        auto cloned = std::make_unique<frontend::VarDecl>(
            varDecl->type,
            varDecl->name,
            cloneExpr(varDecl->initializer.get())
        );
        cloned->is_const = varDecl->is_const;
        cloned->is_stack = varDecl->is_stack;
        cloned->is_wild = varDecl->is_wild;
        cloned->is_wildx = varDecl->is_wildx;
        cloned->generic_params = varDecl->generic_params;
        return cloned;
    }
    
    // Expression statement
    if (auto* exprStmt = dynamic_cast<frontend::ExpressionStmt*>(stmt)) {
        return std::make_unique<frontend::ExpressionStmt>(cloneExpr(exprStmt->expression.get()));
    }
    
    // If statement
    if (auto* ifStmt = dynamic_cast<frontend::IfStmt*>(stmt)) {
        return std::make_unique<frontend::IfStmt>(
            cloneExpr(ifStmt->condition.get()),
            cloneBlock(ifStmt->then_block.get()),
            cloneBlock(ifStmt->else_block.get())
        );
    }
    
    // Block
    if (auto* block = dynamic_cast<frontend::Block*>(stmt)) {
        // Block cloning returns unique_ptr<Block>, need to cast through AstNode
        return std::unique_ptr<frontend::Statement>(dynamic_cast<frontend::Statement*>(cloneBlock(block).release()));
    }
    
    // For other statement types, return nullptr
    return nullptr;
}

// Clone block (deep copy)
std::unique_ptr<frontend::Block> Monomorphizer::cloneBlock(frontend::Block* block) {
    if (!block) return nullptr;
    
    auto new_block = std::make_unique<frontend::Block>();
    
    for (const auto& stmt : block->statements) {
        // Block->statements are AstNode*, try to cast to Statement*
        if (auto* statement = dynamic_cast<frontend::Statement*>(stmt.get())) {
            new_block->statements.push_back(cloneStmt(statement));
        }
    }
    
    return new_block;
}

// Clone function declaration
std::unique_ptr<frontend::FuncDecl> Monomorphizer::cloneFuncDecl(frontend::FuncDecl* original) {
    if (!original) return nullptr;
    
    // Deep clone parameters (FuncParam has unique_ptr, not copyable)
    std::vector<frontend::FuncParam> cloned_params;
    for (const auto& param : original->parameters) {
        cloned_params.emplace_back(
            param.type,
            param.name,
            param.default_value ? cloneExpr(param.default_value.get()) : nullptr
        );
    }
    
    // FuncDecl constructor requires name, generics, parameters, return_type, body
    auto cloned = std::make_unique<frontend::FuncDecl>(
        original->name,
        original->generics,
        std::move(cloned_params),
        original->return_type,
        cloneBlock(original->body.get())
    );
    
    // Copy additional properties
    cloned->is_pub = original->is_pub;
    cloned->is_async = original->is_async;
    cloned->auto_wrap = original->auto_wrap;
    
    // Clone body
    cloned->body = cloneBlock(original->body.get());
    
    return cloned;
}

// Get or create specialized function
std::string Monomorphizer::getOrCreateSpecialization(
    const std::string& trait_name,
    const std::string& type_name,
    const std::string& method_name
) {
    // Check if specialization already exists
    auto key = std::make_tuple(trait_name, type_name, method_name);
    auto it = context.specialization_map.find(key);
    
    if (it != context.specialization_map.end()) {
        return it->second;  // Return existing specialization name
    }
    
    // Find the impl for this trait and type
    auto range = context.impl_table.equal_range(trait_name);
    frontend::ImplDecl* target_impl = nullptr;
    
    for (auto impl_it = range.first; impl_it != range.second; ++impl_it) {
        if (impl_it->second->type_name == type_name) {
            target_impl = impl_it->second;
            break;
        }
    }
    
    if (!target_impl) {
        std::stringstream ss;
        ss << "No implementation of trait '" << trait_name 
           << "' found for type '" << type_name << "'";
        throw std::runtime_error(ss.str());
    }
    
    // Find the method in the impl
    frontend::FuncDecl* method = nullptr;
    for (const auto& m : target_impl->methods) {
        if (m->name == method_name) {
            method = m.get();
            break;
        }
    }
    
    if (!method) {
        std::stringstream ss;
        ss << "Method '" << method_name << "' not found in impl of trait '" 
           << trait_name << "' for type '" << type_name << "'";
        throw std::runtime_error(ss.str());
    }
    
    // Generate specialized name
    std::string specialized_name = generateSpecializedName(trait_name, type_name, method_name);
    
    // Clone the method
    auto specialized_func = cloneFuncDecl(method);
    specialized_func->name = specialized_name;
    
    // Register specialization
    context.specialization_map[key] = specialized_name;
    context.specialized_functions.push_back(std::move(specialized_func));
    
    return specialized_name;
}

// Monomorphize all implementations
std::vector<frontend::FuncDecl*> Monomorphizer::monomorphizeAll() {
    std::vector<frontend::FuncDecl*> result;
    
    // For each implementation, create specializations for all methods
    for (const auto& [trait_name, impl] : context.impl_table) {
        for (const auto& method : impl->methods) {
            std::string specialized_name = getOrCreateSpecialization(
                trait_name,
                impl->type_name,
                method->name
            );
            
            // Find the specialized function we just created
            for (const auto& func : context.specialized_functions) {
                if (func->name == specialized_name) {
                    result.push_back(func.get());
                    break;
                }
            }
        }
    }
    
    return result;
}

} // namespace backend
} // namespace aria



--- src/backend/vtable.h ---
/**
 * src/backend/vtable.h
 *
 * Virtual Table Generation for Trait Dynamic Dispatch
 * 
 * Generates vtables for trait objects and implements fat pointer representation.
 */

#ifndef ARIA_BACKEND_VTABLE_H
#define ARIA_BACKEND_VTABLE_H

#include "../frontend/ast.h"
#include "../frontend/ast/stmt.h"
#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/LLVMContext.h>
#include <llvm/IR/Module.h>
#include <memory>
#include <string>
#include <map>
#include <vector>

namespace aria {
namespace backend {

// Vtable layout for a trait
struct VtableLayout {
    std::string trait_name;
    std::vector<std::string> method_names;  // Ordered list of method names
    std::map<std::string, size_t> method_indices;  // method_name -> index in vtable
};

// Fat pointer structure for trait objects
// Represented in LLVM as: { i8* data, vtable_type* vtable }
struct TraitObjectLayout {
    std::string trait_name;
    llvm::StructType* llvm_type;  // LLVM struct type for the fat pointer
    llvm::StructType* vtable_type;  // LLVM struct type for the vtable
};

// Vtable generator
class VtableGenerator {
private:
    llvm::LLVMContext& llvm_context;
    llvm::Module& llvm_module;
    llvm::IRBuilder<>& builder;
    
    // Map from trait name to vtable layout
    std::map<std::string, VtableLayout> vtable_layouts;
    
    // Map from trait name to fat pointer layout
    std::map<std::string, TraitObjectLayout> trait_object_layouts;
    
    // Map from (trait_name, type_name) to vtable global variable
    std::map<std::pair<std::string, std::string>, llvm::GlobalVariable*> vtable_instances;
    
    // Trait and impl tables
    std::map<std::string, frontend::TraitDecl*> trait_table;
    std::multimap<std::string, frontend::ImplDecl*> impl_table;
    
    // Create LLVM function pointer type for a trait method
    llvm::FunctionType* createMethodFunctionType(
        const frontend::TraitMethod& method,
        llvm::Type* self_type
    );
    
public:
    VtableGenerator(
        llvm::LLVMContext& ctx,
        llvm::Module& mod,
        llvm::IRBuilder<>& b
    ) : llvm_context(ctx), llvm_module(mod), builder(b) {}
    
    // Register a trait declaration
    void registerTrait(frontend::TraitDecl* trait);
    
    // Register a trait implementation
    void registerImpl(frontend::ImplDecl* impl);
    
    // Generate vtable layout for a trait
    VtableLayout generateVtableLayout(const std::string& trait_name);
    
    // Generate vtable LLVM type for a trait
    llvm::StructType* generateVtableType(const std::string& trait_name);
    
    // Generate fat pointer type for a trait object
    llvm::StructType* generateTraitObjectType(const std::string& trait_name);
    
    // Generate vtable instance for a specific trait/type combination
    llvm::GlobalVariable* generateVtableInstance(
        const std::string& trait_name,
        const std::string& type_name
    );
    
    // Create a trait object (fat pointer) from a concrete value
    // Returns LLVM value of type { i8* data, vtable* vtable }
    llvm::Value* createTraitObject(
        llvm::Value* concrete_value,
        const std::string& concrete_type,
        const std::string& trait_name
    );
    
    // Call a method on a trait object (dynamic dispatch)
    llvm::Value* callTraitMethod(
        llvm::Value* trait_object,
        const std::string& trait_name,
        const std::string& method_name,
        const std::vector<llvm::Value*>& args
    );
    
    // Generate all vtables for all registered implementations
    void generateAllVtables();
};

} // namespace backend
} // namespace aria

#endif // ARIA_BACKEND_VTABLE_H



--- src/backend/vtable.cpp ---
/**
 * src/backend/vtable.cpp
 *
 * Virtual Table Generation Implementation
 */

#include "vtable.h"
#include <llvm/IR/DerivedTypes.h>
#include <llvm/IR/Constants.h>
#include <sstream>
#include <stdexcept>

namespace aria {
namespace backend {

// Register trait
void VtableGenerator::registerTrait(frontend::TraitDecl* trait) {
    trait_table[trait->name] = trait;
}

// Register implementation
void VtableGenerator::registerImpl(frontend::ImplDecl* impl) {
    impl_table.insert({impl->trait_name, impl});
}

// Generate vtable layout for a trait
VtableLayout VtableGenerator::generateVtableLayout(const std::string& trait_name) {
    // Check if already generated
    auto it = vtable_layouts.find(trait_name);
    if (it != vtable_layouts.end()) {
        return it->second;
    }
    
    // Find trait declaration
    auto trait_it = trait_table.find(trait_name);
    if (trait_it == trait_table.end()) {
        throw std::runtime_error("Trait not found: " + trait_name);
    }
    
    frontend::TraitDecl* trait = trait_it->second;
    
    VtableLayout layout;
    layout.trait_name = trait_name;
    
    // Collect all methods from trait and super traits
    std::function<void(frontend::TraitDecl*)> collect_methods;
    collect_methods = [&](frontend::TraitDecl* t) {
        // Add methods from super traits first (inheritance order)
        for (const auto& super_name : t->super_traits) {
            auto super_it = trait_table.find(super_name);
            if (super_it != trait_table.end()) {
                collect_methods(super_it->second);
            }
        }
        
        // Add methods from this trait
        for (const auto& method : t->methods) {
            // Check for duplicate (override from super trait)
            if (layout.method_indices.find(method.name) == layout.method_indices.end()) {
                size_t index = layout.method_names.size();
                layout.method_names.push_back(method.name);
                layout.method_indices[method.name] = index;
            }
        }
    };
    
    collect_methods(trait);
    
    // Cache layout
    vtable_layouts[trait_name] = layout;
    
    return layout;
}

// Create LLVM function type for a trait method
llvm::FunctionType* VtableGenerator::createMethodFunctionType(
    const frontend::TraitMethod& method,
    llvm::Type* self_type
) {
    // TODO: Implement proper type mapping from Aria types to LLVM types
    // For now, use placeholder i8* for all types
    
    std::vector<llvm::Type*> param_types;
    
    // First parameter is always 'self' (pointer to the concrete type)
    param_types.push_back(self_type);
    
    // Add remaining parameters
    for (const auto& param : method.parameters) {
        param_types.push_back(llvm::PointerType::get(llvm_context, 0));
    }
    
    // Determine return type
    llvm::Type* return_type;
    if (method.return_type == "void") {
        return_type = llvm::Type::getVoidTy(llvm_context);
    } else {
        return_type = llvm::PointerType::get(llvm_context, 0);
    }
    
    return llvm::FunctionType::get(return_type, param_types, false);
}

// Generate vtable LLVM type
llvm::StructType* VtableGenerator::generateVtableType(const std::string& trait_name) {
    VtableLayout layout = generateVtableLayout(trait_name);
    
    // Create struct type with function pointers
    std::vector<llvm::Type*> method_ptr_types;
    
    // Get trait declaration for method signatures
    auto trait_it = trait_table.find(trait_name);
    if (trait_it == trait_table.end()) {
        throw std::runtime_error("Trait not found: " + trait_name);
    }
    
    frontend::TraitDecl* trait = trait_it->second;
    
    // For each method in vtable layout, create function pointer type
    for (const auto& method_name : layout.method_names) {
        // Find method signature in trait
        const frontend::TraitMethod* method_sig = nullptr;
        for (const auto& m : trait->methods) {
            if (m.name == method_name) {
                method_sig = &m;
                break;
            }
        }
        
        if (!method_sig) {
            throw std::runtime_error("Method not found in trait: " + method_name);
        }
        
        // Create function type (all methods take self as i8*)
        llvm::Type* self_type = llvm::PointerType::get(llvm_context, 0);
        llvm::FunctionType* func_type = createMethodFunctionType(*method_sig, self_type);
        
        // Create function pointer type
        llvm::PointerType* func_ptr_type = llvm::PointerType::get(func_type, 0);
        method_ptr_types.push_back(func_ptr_type);
    }
    
    // Create struct type for vtable
    std::string vtable_name = "vtable_" + trait_name;
    llvm::StructType* vtable_type = llvm::StructType::create(
        llvm_context,
        method_ptr_types,
        vtable_name
    );
    
    return vtable_type;
}

// Generate fat pointer type for trait object
llvm::StructType* VtableGenerator::generateTraitObjectType(const std::string& trait_name) {
    // Check cache
    auto it = trait_object_layouts.find(trait_name);
    if (it != trait_object_layouts.end()) {
        return it->second.llvm_type;
    }
    
    // Generate vtable type
    llvm::StructType* vtable_type = generateVtableType(trait_name);
    
    // Create fat pointer struct: { i8* data, vtable* vtable }
    std::vector<llvm::Type*> fat_ptr_fields = {
        llvm::PointerType::get(llvm_context, 0),  // data pointer (opaque ptr)
        llvm::PointerType::get(vtable_type, 0)   // vtable pointer
    };
    
    std::string fat_ptr_name = "trait_object_" + trait_name;
    llvm::StructType* fat_ptr_type = llvm::StructType::create(
        llvm_context,
        fat_ptr_fields,
        fat_ptr_name
    );
    
    // Cache layout
    TraitObjectLayout layout;
    layout.trait_name = trait_name;
    layout.llvm_type = fat_ptr_type;
    layout.vtable_type = vtable_type;
    trait_object_layouts[trait_name] = layout;
    
    return fat_ptr_type;
}

// Generate vtable instance for specific trait/type
llvm::GlobalVariable* VtableGenerator::generateVtableInstance(
    const std::string& trait_name,
    const std::string& type_name
) {
    // Check cache
    auto key = std::make_pair(trait_name, type_name);
    auto it = vtable_instances.find(key);
    if (it != vtable_instances.end()) {
        return it->second;
    }
    
    // Get vtable type
    llvm::StructType* vtable_type = generateVtableType(trait_name);
    
    // Get vtable layout
    VtableLayout layout = generateVtableLayout(trait_name);
    
    // Find implementation
    auto range = impl_table.equal_range(trait_name);
    frontend::ImplDecl* target_impl = nullptr;
    
    for (auto impl_it = range.first; impl_it != range.second; ++impl_it) {
        if (impl_it->second->type_name == type_name) {
            target_impl = impl_it->second;
            break;
        }
    }
    
    if (!target_impl) {
        std::stringstream ss;
        ss << "No implementation of trait '" << trait_name 
           << "' for type '" << type_name << "'";
        throw std::runtime_error(ss.str());
    }
    
    // Create vtable initializer with method pointers
    std::vector<llvm::Constant*> method_ptrs;
    
    for (const auto& method_name : layout.method_names) {
        // Find method in implementation
        llvm::Function* method_func = nullptr;
        
        // Generate specialized method name
        std::string specialized_name = trait_name + "_" + type_name + "_" + method_name;
        
        // Look up function in module
        method_func = llvm_module.getFunction(specialized_name);
        
        if (!method_func) {
            // Method not yet compiled - create a placeholder
            // This will be filled in during code generation
            throw std::runtime_error("Method function not found: " + specialized_name);
        }
        
        method_ptrs.push_back(method_func);
    }
    
    // Create vtable constant
    llvm::Constant* vtable_init = llvm::ConstantStruct::get(vtable_type, method_ptrs);
    
    // Create global variable for vtable
    std::string vtable_var_name = "vtable_" + trait_name + "_" + type_name;
    llvm::GlobalVariable* vtable_var = new llvm::GlobalVariable(
        llvm_module,
        vtable_type,
        true,  // isConstant
        llvm::GlobalValue::InternalLinkage,
        vtable_init,
        vtable_var_name
    );
    
    // Cache vtable instance
    vtable_instances[key] = vtable_var;
    
    return vtable_var;
}

// Create trait object from concrete value
llvm::Value* VtableGenerator::createTraitObject(
    llvm::Value* concrete_value,
    const std::string& concrete_type,
    const std::string& trait_name
) {
    // Get fat pointer type
    llvm::StructType* fat_ptr_type = generateTraitObjectType(trait_name);
    
    // Get vtable for this type
    llvm::GlobalVariable* vtable = generateVtableInstance(trait_name, concrete_type);
    
    // Allocate fat pointer on stack
    llvm::Value* fat_ptr = builder.CreateAlloca(fat_ptr_type, nullptr, "trait_obj");
    
    // Cast concrete value to i8*
    llvm::Value* data_ptr = builder.CreateBitCast(
        concrete_value,
        llvm::PointerType::get(llvm_context, 0),
        "data_ptr"
    );
    
    // Store data pointer in fat pointer
    llvm::Value* data_field_ptr = builder.CreateStructGEP(fat_ptr_type, fat_ptr, 0, "data_field");
    builder.CreateStore(data_ptr, data_field_ptr);
    
    // Store vtable pointer in fat pointer
    llvm::Value* vtable_field_ptr = builder.CreateStructGEP(fat_ptr_type, fat_ptr, 1, "vtable_field");
    builder.CreateStore(vtable, vtable_field_ptr);
    
    return builder.CreateLoad(fat_ptr_type, fat_ptr, "trait_object");
}

// Call method on trait object (dynamic dispatch)
llvm::Value* VtableGenerator::callTraitMethod(
    llvm::Value* trait_object,
    const std::string& trait_name,
    const std::string& method_name,
    const std::vector<llvm::Value*>& args
) {
    // Get vtable layout
    VtableLayout layout = generateVtableLayout(trait_name);
    
    // Get method index
    auto method_it = layout.method_indices.find(method_name);
    if (method_it == layout.method_indices.end()) {
        throw std::runtime_error("Method not found in trait: " + method_name);
    }
    size_t method_index = method_it->second;
    
    // Get fat pointer type
    llvm::StructType* fat_ptr_type = generateTraitObjectType(trait_name);
    
    // Extract vtable pointer from trait object
    llvm::Value* vtable_ptr = builder.CreateExtractValue(trait_object, 1, "vtable_ptr");
    
    // Extract data pointer from trait object
    llvm::Value* data_ptr = builder.CreateExtractValue(trait_object, 0, "data_ptr");
    
    // Get method pointer from vtable
    llvm::Value* method_ptr_ptr = builder.CreateStructGEP(
        trait_object_layouts[trait_name].vtable_type,
        vtable_ptr,
        method_index,
        "method_ptr_ptr"
    );
    
    llvm::Value* method_ptr = builder.CreateLoad(
        llvm::PointerType::get(llvm_context, 0),  // Opaque pointer type
        method_ptr_ptr,
        "method_ptr"
    );
    
    // Prepare arguments: self + args
    std::vector<llvm::Value*> call_args = {data_ptr};
    call_args.insert(call_args.end(), args.begin(), args.end());
    
    // Call method through function pointer
    // In LLVM 20 with opaque pointers, CreateCall infers the function type
    // We need to get the function type from the trait method signature
    llvm::FunctionType* func_type = llvm::FunctionType::get(
        llvm::PointerType::get(llvm_context, 0),  // return type (generic ptr)
        {llvm::PointerType::get(llvm_context, 0)},  // self parameter
        false  // not vararg
    );
    
    return builder.CreateCall(
        func_type,
        method_ptr,
        call_args,
        "trait_method_call"
    );
}

// Generate all vtables
void VtableGenerator::generateAllVtables() {
    // For each implementation, generate a vtable instance
    for (const auto& [trait_name, impl] : impl_table) {
        generateVtableInstance(trait_name, impl->type_name);
    }
}

} // namespace backend
} // namespace aria



--- Runtime Headers (Key Interfaces) ---

--- src/runtime/gc/header.h ---
#include <cstdint>
#include <cstddef>

// Type IDs for runtime type information
enum TypeID {
   TYPE_INT = 0,
   TYPE_TRIT = 1,
   TYPE_ARRAY_OBJ = 2,
   TYPE_STRUCT = 3
};

struct ObjHeader {
   // Bitfields for compact storage overhead (8 bytes total)
   uint64_t mark_bit : 1;      // Used by Mark-and-Sweep algorithm
   uint64_t pinned_bit : 1;    // The '#' Pinning Flag. If 1, GC skips moving this.
   uint64_t forwarded_bit : 1; // Used during Copying phase to track relocation
   uint64_t is_nursery : 1;    // Generation flag (0=Old, 1=Nursery)
   uint64_t size_class : 8;    // Allocator size bucket index
   uint64_t type_id : 16;      // RTTI / Type information for 'dyn' and pattern matching
   uint64_t padding : 36;      // Reserved for future use (e.g., hash code cache)
};

// Fragment structure for managing free blocks in nursery
struct Fragment {
   char* start;
   size_t size;
   Fragment* next;
};

// Nursery structure for generational GC
struct Nursery {
   char* start_addr;     // Start of nursery memory region
   char* bump_ptr;       // Current allocation pointer
   char* end_addr;       // End of nursery memory region
   Fragment* fragments;  // Free list for fragmented space
};



--- src/runtime/gc/shadow_stack.h ---
#ifndef ARIA_SHADOW_STACK_H
#define ARIA_SHADOW_STACK_H

#include <vector>

// Shadow Stack API for GC Root Tracking
// 
// The shadow stack maintains a parallel stack of GC-managed pointers,
// enabling precise root identification during garbage collection.
//
// Usage Pattern (in generated LLVM IR):
//   func:example = void() {
//       aria_shadow_stack_push_frame();     // Function entry
//       
//       auto:obj = gc_alloc(...);
//       aria_shadow_stack_add_root(&obj);   // Register GC pointer
//       
//       // ... use obj ...
//       
//       aria_shadow_stack_remove_root(&obj); // Unregister (optional - frame pop does this)
//       aria_shadow_stack_pop_frame();       // Function exit
//   }

#ifdef __cplusplus
extern "C" {
#endif

// Frame Management
// ----------------
// Push a new shadow stack frame (call at function entry)
void aria_shadow_stack_push_frame();

// Pop the current shadow stack frame (call at function exit)
void aria_shadow_stack_pop_frame();

// Root Registration
// -----------------
// Add a GC root pointer to the current frame
// ptr_addr: address of the local variable holding the GC pointer
void aria_shadow_stack_add_root(void** ptr_addr);

// Remove a GC root pointer from the current frame
// ptr_addr: address of the local variable being destroyed
void aria_shadow_stack_remove_root(void** ptr_addr);

// Thread Lifecycle
// ----------------
// Initialize shadow stack for current thread (optional - lazy init)
void aria_shadow_stack_init();

// Cleanup shadow stack on thread exit
void aria_shadow_stack_cleanup();

#ifdef __cplusplus
}

// C++ API for GC integration
// ---------------------------
// Get all GC roots from the shadow stack (called during collection)
std::vector<void*> aria_shadow_stack_get_roots();

#endif

#endif // ARIA_SHADOW_STACK_H



--- src/runtime/memory/allocator.h ---
#ifndef ARIA_RUNTIME_ALLOCATOR_H
#define ARIA_RUNTIME_ALLOCATOR_H

#include <stddef.h>

#ifdef __cplusplus
extern "C" {
#endif

// Runtime Interface for Wild Allocations
// These functions are linked directly to the 'aria.alloc' and 'aria.free' intrinsics.

// Basic allocation mapping
void* aria_alloc(size_t size);

// Explicit deallocation
void aria_free(void* ptr);

// Reallocation
void* aria_realloc(void* ptr, size_t size);

// Aligned allocation for SIMD types (vec9, tensor)
// Ensures pointers respect the 64-byte alignment required by AVX-512 ZMM registers
void* aria_alloc_aligned(size_t size, size_t alignment);

#ifdef __cplusplus
}
#endif

#endif // ARIA_RUNTIME_ALLOCATOR_H



--- src/runtime/concurrency/scheduler.h ---
#include <vector>
#include <deque>
#include <thread>
#include <mutex>

struct CoroutineFrame; // Forward decl

// Coroutine state constants
#define CORO_RUNNING 0
#define CORO_SUSPENDED 1
#define CORO_COMPLETE 2

// Task represents a suspended coroutine
struct Task {
   CoroutineFrame* frame;
   bool has_wild_affinity; // If true, cannot be stolen by other workers
   int affinity_thread_id; 
};
// Worker represents an OS thread
struct Worker {
   int id;
   std::deque<Task*> local_queue;
   // The Work-Stealing Deque
   std::mutex queue_lock;         // Spinlock for steal operations
   void run();
};
// Global Scheduler Context
struct Scheduler {
   std::vector<std::thread> workers;
   std::vector<Worker*> queues;
   // Global lock only used during runtime initialization/shutdown
   std::mutex init_mutex;
   // Helper to push task to current thread's queue
   void schedule(Task* t) {
       // Implementation details omitted for brevity
   }
};

// RAMP: Coroutine Frame definition
struct CoroutineFrame {
    void* coro_handle;     // LLVM coroutine handle (opaque ptr from llvm.coro.begin)
    void* data;            // Captured state (promoted from stack)
    CoroutineFrame* waiting_on; 
    int state;             // RUNNING, SUSPENDED, COMPLETE
    char padding;      // Alignment for AVX
};

// Bridge function for resuming LLVM coroutines
// Called by scheduler, internally invokes llvm.coro.resume
extern "C" void aria_coro_resume_bridge(void* coro_handle);

// Allocate and free CoroutineFrame structs
extern "C" CoroutineFrame* aria_frame_alloc();
extern "C" void aria_frame_free(CoroutineFrame* frame);

// Scheduler C API
extern "C" void aria_scheduler_init(int num_threads);
extern "C" void aria_scheduler_shutdown();
extern "C" void aria_scheduler_schedule(CoroutineFrame* frame);
extern "C" void aria_scheduler_resume(CoroutineFrame* frame);




--- src/runtime/concurrency/spawn.h ---
#pragma once
#include <mutex>
#include <condition_variable>
#include <atomic>
#include <cstdint>
#include <cstring>

// Future - type-erased result container for spawned tasks
// Uses void* storage to avoid template instantiation in C
struct Future {
    void* result;                       // Pointer to result value (heap allocated)
    std::atomic<bool> completed;        // Has task finished?
    std::mutex mutex;                   // Protects result access
    std::condition_variable cv;         // For efficient blocking in get()
    size_t result_size;                 // Size of result type for allocation
    
    Future(size_t size) : result(nullptr), completed(false), result_size(size) {
        if (size > 0) {
            result = malloc(size);
            memset(result, 0, size);
        }
    }
    
    ~Future() {
        if (result) {
            free(result);
        }
    }
    
    // Block until result is ready
    void wait() {
        std::unique_lock<std::mutex> lock(mutex);
        while (!completed.load()) {
            cv.wait(lock);
        }
    }
    
    // Called by worker thread when task completes
    void set(void* value);
    
    // Get result pointer (caller must cast to correct type)
    void* get() {
        wait();
        return result;
    }
};

// SpawnTask - simpler than CoroutineFrame, just a function call
struct SpawnTask {
    void (*function)(void*);   // The function to execute
    void* args;                // Arguments bundled as struct
    Future* future;            // Future to write result into
    
    // Type-specific completion that knows how to extract result
    // Signature: void completion(void* future, void* result)
    void (*completion)(void* future, void* result);
};

// C API for spawning tasks
extern "C" {
    // Schedule a spawn task (simpler than coroutine scheduling)
    void aria_spawn_schedule(SpawnTask* task);
    
    // Initialize spawn runtime (may reuse coroutine scheduler)
    void aria_spawn_init(int num_threads);
    
    // Shutdown spawn runtime
    void aria_spawn_shutdown();
    
    // Allocate a Future for a given type size
    Future* aria_future_create(size_t result_size);
    
    // Wait for Future and get result pointer
    void* aria_future_get(Future* future);
    
    // Free a Future
    void aria_future_free(Future* future);
}
