Aria Compiler Runtime Systems and Optimizations: Comprehensive Implementation Report
1. Executive Summary
This report serves as the definitive architectural and implementation guide for "Work Package #003" of the Aria Compiler project. It addresses the critical "last mile" integration of three subsystems: the Asynchronous Scheduler, SIMD Vector Operations, and the Generational Garbage Collector (GC) Nursery. These systems, currently existing as disconnected components or partial implementations, represent the foundational pillars of the Aria runtime environment.1
The analysis provided herein is exhaustive, extending beyond mere code patching to encompass the theoretical underpinnings, architectural trade-offs, and hardware-level implications of each design decision. The objective is to transition the Aria runtime from a functional prototype to a production-grade execution engine capable of supporting the language's advanced features, such as wild memory management, twisted balanced binary (TBB) arithmetic, and stackless coroutines.1
The report is structured into three primary technical sections. Section 2 details the Async/Await integration, proposing a novel "Resume Thunk" mechanism to bridge LLVM intrinsics with the work-stealing scheduler, alongside a cache-optimized Task<T> state machine. Section 3 addresses SIMD vectorization, defining the lowering logic for vector arithmetic and swizzling to leverage SSE/AVX hardware, ensuring Aria's mathematical performance matches dedicated shader languages. Section 4 specifies the GC Nursery connection, mandating a Thread-Local Storage (TLS) architecture and a shadow stack mechanism to activate the generational collector, thereby resolving the current inefficient bypass to mi_malloc.
Each section synthesizes requirements from the provided work package 1 and language specifications 1, offering step-by-step implementation plans, rigorous testing strategies, and comparative analyses against established systems like Rust, Go, and GLSL.
________________
2. Issue 1: Async/Await Scheduler Integration
The implementation of asynchronous programming in Aria is currently in a "disconnected" state. While the frontend successfully lowers async functions to LLVM coroutines using the llvm.coro intrinsic family, and the runtime possesses a functional work-stealing scheduler, the cohesive link between suspension and resumption is absent.1 This section details the mechanisms required to bridge this gap, ensuring robust concurrency.
2.1 Architectural Analysis of the Coroutine Gap
The fundamental disconnect lies in the interface mismatch between the code generated by LLVM's coroutine passes and the expectations of the Aria runtime scheduler.
2.1.1 The LLVM Coroutine Model
LLVM lowers coroutines using a "switch-resume" technique. A coroutine is split into three logical parts:
1. The Ramp Function: The initial entry point that allocates the coroutine frame and executes until the first suspension point.
2. The Resume Function: A sub-function that restores state and continues execution.
3. The Destroy Function: A sub-function responsible for cleaning up the frame.
Crucially, the llvm.coro.suspend intrinsic returns a token indicating whether the coroutine is suspending, destroying, or returning. However, upon suspension, the current implementation simply returns the frame pointer to the caller and ceases operation. It fails to register the suspended coroutine with any scheduling entity. Consequently, the coroutine enters a "zombie" state—suspended but effectively leaked, as no mechanism exists to trigger its resumption.
2.1.2 The Runtime Scheduler Expectations
The existing scheduler in src/runtime/concurrency/scheduler.cpp operates on a Task abstraction (or raw CoroutineFrame). It expects a callback function—resume_pc—to execute a task.1 This callback typically has the signature void (*)(void*).
The mismatch is evident: LLVM does not expose the "Resume Function" as a standard C-compatible function pointer that can be directly stored in a scheduler's queue. Instead, resumption is triggered via the llvm.coro.resume intrinsic, which the backend lowers to a direct jump or indirect call based on the frame's internal vtable.
2.2 The Bridging Solution: Thunks and Tasks
To resolve this, we must introduce two intermediate layers: a compiler-generated "Resume Thunk" to normalize the function signature, and a runtime Task<T> wrapper to manage the state machine and data flow.
2.2.1 The Resume Thunk Implementation
The scheduler cannot directly call LLVM intrinsics. Therefore, for every async function compiled, the codegen must synthesize a static trampoline function, or "Thunk."
Design:
The Thunk accepts a generic void* pointer (the coroutine frame) and internally calls the llvm.coro.resume intrinsic. This satisfies the scheduler's need for a simple function pointer while encapsulating the complexity of LLVM's coroutine ABI.
Implementation Plan (CodeGen):
In src/backend/codegen.cpp, during visit(AsyncFuncDecl):
1. Generate Body: Compile the user's async function body normally.
2. Synthesize Thunk: Create a new Function named OriginalName_resume_thunk.
3. Thunk Logic:
   * Cast the argument to i8* (the frame handle).
   * Call @llvm.coro.resume(i8* %handle).
   * Return void.
4. Registration: Store the address of this Thunk in the Task structure associated with the coroutine.
This approach decouples the runtime from the compiler version. Even if LLVM's coroutine lowering strategy changes in future versions, the Thunk provides a stable ABI for the scheduler.
2.2.2 The Task<T> Type Design
The prompt 1 highlights the need for a Task<T> wrapper, akin to Rust's Future or JavaScript's Promise. This structure is the synchronization primitive that facilitates the await mechanism.
Memory Layout & Alignment:
Efficiency is paramount. The Task struct is a "hot" object, frequently accessed by multiple threads (the producer and the consumer).
Offset
	Field
	Type
	Description
	0
	state
	atomic<uint8_t>
	Current status (PENDING, RUNNING, COMPLETED).
	1
	flags
	uint8_t
	Bitmask for properties (e.g., HAS_WILD_AFFINITY).
	8
	coro_handle
	void*
	Pointer to the coroutine frame (for the task itself).
	16
	resume_fn
	func_ptr
	The "Resume Thunk" for this task.
	24
	awaiter_handle
	void*
	Pointer to the waiting coroutine's frame.
	32
	awaiter_resume_fn
	func_ptr
	The "Resume Thunk" for the waiting coroutine.
	40
	result
	Union<T, Error>
	Storage for the return value or error.
	64
	padding
	byte
	Pad to 64 bytes to prevent false sharing on cache lines.
	Rationale:
* Atomic State: Allows lock-free checks for completion.
* Continuation Storage (awaiter_*): Crucial for the await mechanism. When Task A awaits Task B, Task A registers itself here.
* Result Union: Aria uses Result<T> patterns.1 The task must store either the value or the error.
2.3 Detailed Integration Logic: visit(AwaitExpr)
The visit(AwaitExpr) function is the nexus of the integration. The current implementation suspends but fails to schedule. The corrected logic must implement a "continuation-passing" style.
Step-by-Step Implementation Algorithm:
1. Evaluation: Compile the expression being awaited (the Task object). Let's call this targetTask.
2. Fast Path Check (Optimization):
   * Generate code to check targetTask->state.
   * If state == COMPLETED, bypass suspension entirely. Jump directly to result extraction. This mimics "eager" execution.
3. Suspend Path Preparation:
   * Call llvm.coro.save to get the save token.
   * Call llvm.coro.frame to get the current coroutine's handle.
4. Suspension:
   * Call llvm.coro.suspend.
   * Generate the 3-way switch (Suspend, Resume, Cleanup).
5. The Critical Fix (Suspend Block):
   * In the suspend block (which executes before returning to the caller), we perform the hand-off.
   * Call Runtime: aria_task_register_awaiter(targetTask, currentFrame, currentResumeThunk).
   * Thread Safety: This runtime function must atomically check if targetTask is already done. If so, it must return a signal to resume immediately rather than suspend. Assuming it's not done, it stores the currentFrame as the continuation.
   * Yield: Return the currentFrame handle (or nullptr) to the caller, effectively yielding control to the scheduler.
6. Resumption (Resume Block):
   * This block is reached when the scheduler invokes the Resume Thunk.
   * Extract Result: Call aria_task_get_result(targetTask).
   * Error Handling: Check if the result indicates an error. If Aria's await semantics require propagation, generate the branch to return the error immediately.
2.4 Wild Affinity and Thread Pinning
The requirement for "Wild Affinity" 1 is unique to Aria. The wild keyword allows unmanaged memory allocation. If a coroutine allocates thread-local wild memory (e.g., using a thread-local arena), it cannot be moved to another thread.
Implementation Strategy:
1. Detection: The compiler tracks usage of wild allocations. If visit(WildAlloc) is encountered within an async function scope, the function is marked with an attribute PreserveAll.
2. Runtime Flag: When creating the Task, if the compiler detected wild usage, it sets the HAS_WILD_AFFINITY flag in the Task header.
3. Scheduler Modification:
   * Standard work-stealing allows a thief to take tasks from the tail of another worker's deque.
   * Constraint: Before stealing, the thief must check the HAS_WILD_AFFINITY flag.
   * If set, the task cannot be stolen. It must remain on the original thread. The thief must search elsewhere.
2.5 Error Handling and Result Propagation
Aria's await operator expects to unwrap values, but the underlying async function returns a Result<T>.
Proposed Semantics:
* await task evaluates to the T inside Task<T>.
* If the async function failed, the Task holds an error.
* Upon resumption, the generated code checks the error state.
* Bubbling: If an error exists, and the current function also returns a Result, the error is implicitly propagated (similar to Rust's ? operator).
* Panic: If the current function cannot return an error, a runtime panic is triggered.
Type System Check (sema/type_checker.cpp):
* Verify that await is only used inside async functions.
* Verify that the operand of await is a Task<T>.
* Verify that the return type of the enclosing async function is compatible with the error type potentially returned by the awaited task.
2.6 Comparative Analysis: Async Implementation
Feature
	Aria (Proposed)
	Rust
	Go
	C++20
	Model
	Stackless Coroutines
	Stackless Coroutines
	Stackful (Green Threads)
	Stackless Coroutines
	Scheduling
	Runtime Work-Stealing
	Executor agnostic (Library)
	Runtime M:N Scheduler
	Library specific
	State
	Task<T> (Allocated)
	Future (Zero-cost state machine)
	Dynamic Stack
	std::coroutine_handle
	Affinity
	Explicit "Wild" Pinning
	Send/Sync traits
	None (Preemptive)
	None (Executor dependent)
	Overhead
	Medium (Task allocation)
	Low (Compiler state machine)
	Medium (Stack checks)
	Low (Heap alloc)
	Aria's approach balances the performance of stackless coroutines with the ease of use of a built-in runtime scheduler, specifically optimized for its unique memory model (wild/gc).
________________
3. Issue 2: SIMD Vector Operations Lowering
Aria defines explicit vector types (vec2, vec3, vec4, etc.) in its specification 1, targeting high-performance graphics and mathematical applications. While the frontend correctly parses these types and the backend maps them to LLVM FixedVectorType, the intermediate lowering of operations is generic, resulting in scalar emulation rather than hardware-accelerated SIMD instructions.1
3.1 The SIMD Lowering Pipeline
The objective is to translate Aria's high-level vector arithmetic into architecture-specific vector instructions (e.g., x86 SSE/AVX or ARM NEON). LLVM provides a platform-independent vector IR, which the backend (LLC) then lowers to machine code.
3.1.1 Type-Aware Binary Operation Generation
The current visit(BinaryOp) implementation 1 naively calls builder->CreateAdd for all types. While CreateAdd supports vectors, it does not handle type mismatches (broadcasting) or the distinction between integer and floating-point logic required by LLVM IR.
The Matrix of Operations:
Aria Op
	Operand A
	Operand B
	LLVM Instruction
	Notes
	+
	vec4
	vec4
	fadd <4 x float>
	Element-wise float add
	+
	ivec4
	ivec4
	add <4 x i32>
	Element-wise int add
	*
	vec4
	float
	fmul <4 x float>
	Requires Broadcasting
	*
	vec4
	vec4
	fmul <4 x float>
	Element-wise multiplication
	%
	ivec4
	ivec4
	srem <4 x i32>
	Signed remainder
	Implementation Logic (codegen.cpp):
The updated visit(BinaryOp) must perform the following steps:
1. Type Detection: Identify if either operand is a vector.
2. Broadcasting (Splatting): If one operand is a scalar and the other a vector, the scalar must be "splatted" into a vector of the same dimension.
   * Mechanism: Use insertelement to place the scalar at index 0, followed by a shufflevector with a zero-mask <0, 0, 0, 0> to replicate it across all lanes.
3. Instruction Selection: Dispatch to the correct LLVM builder method based on the element type (Integer vs. Float).
   * Floats: CreateFAdd, CreateFSub, CreateFMul, CreateFDiv.
   * Ints: CreateAdd, CreateSub, CreateMul, CreateSDiv.
3.1.2 Vector Intrinsics and Geometric Functions
Geometric operations like Dot Product and Cross Product do not have single-instruction equivalents in generic LLVM IR. They must be synthesized from lower-level primitives.
1. Dot Product (dot):
$\text{dot}(\mathbf{a}, \mathbf{b}) = \sum a_i b_i$
* Step 1: Element-wise multiply (fmul).
* Step 2: Horizontal reduction.
   * For vec4, this is a tree reduction.
   * tmp1 = shuffle(mul, mul, <2, 3, undef, undef>)
   * sum1 = fadd(mul, tmp1) (Adds X+Z and Y+W)
   * tmp2 = shuffle(sum1, sum1, <1, undef, undef, undef>)
   * dot = fadd(sum1, tmp2) (Adds (X+Z) + (Y+W))
   * extractelement(dot, 0)
2. Cross Product (cross):
Defined only for vec3.
$\mathbf{a} \times \mathbf{b} = (a_y b_z - a_z b_y, a_z b_x - a_x b_z, a_x b_y - a_y b_x)$
* This is implemented using shufflevector to rearrange components.
   * A_yzx = shuffle(A, <1, 2, 0>)
   * B_yzx = shuffle(B, <1, 2, 0>)
   * A_zxy = shuffle(A, <2, 0, 1>)
   * B_zxy = shuffle(B, <2, 0, 1>)
   * Result = sub( mul(A_yzx, B_zxy), mul(A_zxy, B_yzx) )
* This creates a highly efficient, branchless sequence of vector instructions.
3. Normalization (normalize):
$\text{normalize}(\mathbf{v}) = \mathbf{v} / \sqrt{\text{dot}(\mathbf{v}, \mathbf{v})}$
* Calculates the dot product of v with itself (length squared).
* Computes the square root (llvm.sqrt intrinsic).
* Splats the result.
* Divides v by the splatted length.
3.2 Swizzling Implementation
Swizzling allows rearranging vector components using member syntax (e.g., v.wzyx). This feature, borrowed from GLSL/HLSL, is critical for graphics ergonomics.
Parser Integration (parser_expr.cpp):
The parser must distinguish between standard member access (struct fields) and swizzles.
* Heuristic: If the base expression is a vector type, and the member identifier contains only characters from {x, y, z, w} or {r, g, b, a}, it is a swizzle.
* Validation: The length of the swizzle determines the resulting vector type (e.g., v.xy produces a vec2). Indices must be within bounds (e.g., v.z on a vec2 is a compile error).
Codegen (visit(MemberAccess)):
Swizzling maps directly to shufflevector.
* Input: vec4 v, Swizzle wyx (indices 3, 1, 0).
* IR: shufflevector <4 x float> %v, <4 x float> undef, <3 x i32> <i32 3, i32 1, i32 0>.
* Result: <3 x float>.
3.3 Optimization and Testing
3.3.1 Optimization Guide
* Data Alignment: LLVM assumes standard alignment. For vec4, 16-byte alignment is preferred for SSE/AVX. The compiler should enforce align 16 on vector allocations to allow movaps (aligned move) instead of movups (unaligned move).
* Fused Multiply-Add (FMA): Modern CPUs support FMA (a*b + c). The backend should enable the fma contract to allow the optimizer to merge fmul and fadd sequences.
3.3.2 Testing Strategy
1. Assembly Verification: Write test cases (e.g., vec4 a = b + c) and inspect the generated assembly. Success is defined by the presence of addps (packed single add) or vadd.f32. Failure is indicated by scalar addss instructions in a loop.
2. Edge Cases: Test broadcasting (vec4 * float), mixed swizzles (v.xxyy), and invalid swizzles (parser error checks).
3. Performance Benchmarking: Compare a matrix multiplication implemented with loops vs. dot product intrinsics. The vector implementation should yield a 3-4x speedup on x86-64.
________________
4. Issue 3: GC Nursery Allocator Connection
The Aria memory management model combines manual wild allocation with automatic Garbage Collection (GC) for standard objects. The GC design follows the Generational Hypothesis ("most objects die young"). A "Nursery" (Young Generation) allocator has been implemented but is currently bypassed in favor of mi_malloc.1 This section details the integration of the nursery to enable high-performance, generational collection.
4.1 Theory of Operation: Generational GC
The efficiency of a generational GC relies on the cheap allocation and reclamation of short-lived objects.
1. Allocation: A simple pointer increment ("bump pointer"). Complexity: $O(1)$.
2. Collection: Only live objects are copied out of the nursery. Dead objects are simply overwritten during the next cycle. Complexity: $O(\text{live\_objects})$.
Since most objects die young, the cost of collection is near zero, and allocation is as fast as stack allocation. The current mi_malloc approach (O(log n) or similar complexity with fragmentation handling) is significantly slower for this workload.
4.2 Thread-Local Storage (TLS) Architecture
To support parallel execution without locking on every allocation, each thread must possess its own Nursery.
Implementation Plan:
1. TLS Definition: Declare a thread-local pointer to the Nursery structure.
C++
// src/runtime/gc/nursery.h
extern thread_local Nursery* current_nursery;

2. Initialization:
   * When aria_scheduler_init spawns worker threads, it must allocate a new 4MB Nursery block for each worker.
   * This block is registered with the GC subsystem (so it can be scanned).
   * The current_nursery TLS variable is set to point to this block.
   3. Codegen Access:
   * LLVM's thread_local mode allows accessing this variable.
   * For maximum performance, the compiler should inline the access.
4.3 Inlining the Allocation Fast Path
Calling a function (aria_gc_alloc) for every object incurs overhead (register saving, call frame setup). The standard optimization is to inline the "Fast Path" (the bump pointer check) directly into the user code.
Codegen Logic (visit(AllocExpr)):
The compiler generates the following IR sequence:
   1. Load Pointers: Load bump_ptr and limit_ptr from the TLS Nursery.
   2. Tentative Increment: Calculate new_bump = bump_ptr + size.
   3. Branch: Compare new_bump > limit_ptr.
   * Case A (Fit): Update bump_ptr in TLS. Return old bump_ptr. (Total: ~5 instructions).
   * Case B (Overflow): Call the runtime function aria_gc_alloc_slow. This function handles the collection or fragmentation search.
4.4 The "Fragmented" Nursery and Pinned Objects
Aria supports pin(#) operators 1, which prevent the GC from moving an object. This breaks the standard "Copying GC" model where the nursery is completely evacuated.
Problem: Pinned objects remain in the nursery after collection. The nursery is no longer a single contiguous free block; it is a "Swiss cheese" of free space and pinned objects.
Solution: Hybrid Allocation Strategy
   1. Phase 1: Contiguous Bump: Initially, the nursery is empty. Use standard bump allocation.
   2. Phase 2: Post-Collection:
   * The collector evacuates unpinned objects.
   * It scans the nursery linearly to identify "holes" (ranges of free memory between pinned objects).
   * These holes are linked into a Fragment list.1
   3. Phase 3: Fragmented Allocation:
   * If the fragment list is non-empty, the allocator uses a "First-Fit" strategy from the fragments.
   * Optimization: If fragments are too small, or fragmentation is too high (e.g., >50% pinned), the runtime should "condemn" the page (promote it entirely to Old Gen) and allocate a fresh 4MB nursery block. This restores the fast bump-pointer path.
4.5 Write Barriers and Remembered Sets
To collect the Nursery without scanning the entire Old Generation, the GC must know about pointers from Old to Young objects.
Mechanism: Card Marking
   1. Structure: Divide the heap into 512-byte "cards." Maintain a byte array card_table.
   2. Barrier Logic: Whenever a pointer is written to the heap (obj.field = ptr):
   * Calculate the index of the obj in the card table.
   * Mark that card as "Dirty" (card_table[index] = 1).
   * Note: We mark the source of the pointer, not the target.
   3. Codegen:
   * In visit(AssignmentOp), if the LHS is a GC-managed object and the type is a reference, emit the write barrier code.
   * This is a simple shift and byte store, extremely cheap on modern hardware.
4.6 Stack Scanning: The Shadow Stack
The GC must find roots on the stack. LLVM provides gc.root, but it hinders optimization. The prompt suggests get_thread_roots() is currently a mock.
Implementation: Explicit Shadow Stack
   1. Design: A linked list of StackFrame structures, stored in TLS.
   2. Codegen:
   * Function Entry: Push a StackFrame containing pointers to all local GC variables.
   * Function Exit: Pop the StackFrame.
   * Variable Access: All local GC variables are stored inside this StackFrame (on the actual stack), rather than in arbitrary registers.
   3. Trade-off: This adds overhead to function calls and variable access but guarantees precise rooting without requiring complex DWARF stack map parsing. Given the project's current maturity, this is the most robust solution.
4.7 Comparative Analysis: GC Strategies
Feature
	Aria (Generational)
	Go (Concurrent Mark-Sweep)
	Java (G1/Shenandoah)
	C++ (Manual)
	Throughput
	High (Bump Alloc)
	Medium (Free-list Alloc)
	High
	High
	Latency
	Medium (Stop-the-world minor)
	Low (Concurrent)
	Tunable
	N/A
	Fragmentation
	Low (Compacting)
	Medium
	Low
	High
	Pinning
	Supported (Hybrid Nursery)
	Supported (Non-moving)
	Supported (Region locking)
	N/A
	Aria's hybrid nursery approach allows it to support low-level systems programming (pinning, wild pointers) while maintaining the development velocity of a managed language.
________________
5. Comprehensive Testing & Validation Strategy
Integrating these runtime systems requires a rigorous testing regimen to prevent subtle concurrency bugs and memory corruption.
5.1 Async/Await Testing
   * Sequential Chain Test: verify A await B await C executes in correct order.
   * Concurrency Stress Test: Spawn 10,000 tasks that increment a shared atomic counter. Verify the final count equals 10,000. This tests scheduler throughput and race conditions.
   * Affinity Verification: Create a task with wild allocation. Log the thread ID at start and end. Verify they match, even if other threads are idle.
5.2 SIMD Validation
   * IR Inspection: Compile a vector math snippet. Use FileCheck to scan the LLVM IR for <4 x float> instructions.
   * Numerical Identity: Verify cross(vec3(1,0,0), vec3(0,1,0)) equals vec3(0,0,1).
   * Swizzle Logic: Verify vec4(1,2,3,4).wzyx equals vec4(4,3,2,1).
5.3 GC Stress Testing
   * Nursery Overflow: Allocate 10MB of objects (exceeding 4MB nursery). Verify minor collection triggers.
   * Survival Test: Allocate an object, store it in a global variable, trigger GC. Verify the object is moved to Old Gen and the pointer is updated.
   * Pinning Integrity: Pin an object, trigger GC. Verify the object address has not changed and the data remains valid.
________________
6. Strategic Conclusions
The implementation of Work Package #003 represents a transformative step for the Aria compiler.
   1. Concurrency: The bridge between LLVM coroutines and the work-stealing scheduler moves Aria from a theoretical async model to a high-performance, I/O-ready system. The Task<T> design ensures zero-cost abstractions for state management while supporting the unique "wild affinity" requirement.
   2. Performance: Lowering vector operations to SIMD intrinsics unlocks the hardware's full potential, making Aria a viable candidate for game development and scientific computing, directly competing with C++ and HLSL.
   3. Memory: The generational GC integration replaces a naive malloc wrapper with a state-of-the-art allocation strategy. This will likely improve allocation-heavy benchmark performance by an order of magnitude.
By strictly adhering to the implementation plans detailed in this report, the engineering team can effectively close the gap between the compiler's frontend promises and the runtime's execution capabilities. The resulting system will offer a unique blend of safety, control, and raw performance, fulfilling the core tenets of the Aria language design.
Works cited
   1. GEMINI_WORK_PACKAGE_003.txt