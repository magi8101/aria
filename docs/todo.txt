ARIA COMPILER - COMPLETE IMPLEMENTATION ROADMAP
================================================
Created: December 14, 2025
Version: v0.1.0 - Fresh Start from Archived v0.0.17
Status: Ready for systematic, research-driven implementation

███████████████████████████████████████████████████████████████████████
██ ARCHIVE COMPLETE ✅                                                ██
███████████████████████████████████████████████████████████████████████

v0.0.17 archived to: archive/v0.0.17_20251214_215757/
- All previous source code preserved
- Modular architecture achieved but starting fresh
- Research-driven approach from ground up

███████████████████████████████████████████████████████████████████████
██ RESEARCH FOUNDATION (100% COMPLETE) ✅                             ██
███████████████████████████████████████████████████████████████████████

All 31 research documents completed in: docs/gemini/responses/

research_001 - Borrow Checker - Complete lifetime analysis algorithm
research_002 - Balanced Ternary - TBB arithmetic (tbb8, tbb16, tbb32, tbb64)
research_003 - Balanced Nonary - Nit/nyte arithmetic
research_004 - File I/O Library - Complete file operations
research_005 - Process Management - spawn, fork, exec, pipes
research_006 - Modern Streams - Six-channel I/O system
research_007 - Threading Library - Thread creation and synchronization
research_008 - Atomics Library - Atomic operations
research_009 - Timer/Clock Library - Time and timing functions
research_010-011 - Macro/CompTime - Macro system & compile-time evaluation
research_012 - Standard Integers - int1-512, uint8-512 specifications
research_013 - Floating Point - flt32-512 specifications
research_014 - Composite Part 1 - Arrays, pointers, strings
research_015 - Composite Part 2 - Structs, enums, unions
research_016 - Functional Types - func, result, lambdas, closures
research_017 - Mathematical Types - matrix, tensor, vec9
research_018 - Looping Constructs - while, for, loop, till, when
research_019 - Conditionals - if, pick, pattern matching
research_020 - Control Transfer - break, continue, defer, result monad
research_021 - GC System - Complete garbage collector specification
research_022 - Wild/WildX - Manual and executable memory management
research_023 - Runtime Assembler - JIT compilation for wildx
research_024 - Arithmetic/Bitwise - All arithmetic operators
research_025 - Comparison/Logical - All comparison operators
research_026 - Special Operators - @, $, #, |>, <|, ?., ??, .., etc.
research_027 - Generics - Templates and monomorphization
research_028 - Module System - use, mod, pub, extern
research_029 - Async/Await - Async runtime and futures
research_030 - Const/CompTime - Compile-time evaluation
research_031 - Essential Stdlib - Standard library bootstrap

███████████████████████████████████████████████████████████████████████
██ IMPLEMENTATION PHILOSOPHY                                          ██
███████████████████████████████████████████████████████████████████████

**MODULAR FROM DAY ONE:**
- NO monolithic files (maximum 1000 lines per file)
- One clear responsibility per module
- Clean interfaces between components
- Each module independently testable

**DEVELOPMENT WORKFLOW:**
- All work on `development` branch
- Commit frequently with clear, descriptive messages
- Run tests before each commit
- Only merge to `main` when section is VERIFIED and ALL TESTS PASS
- NEVER lose working code

**RESEARCH-DRIVEN DEVELOPMENT:**
- Each task references specific research document(s): [research_NNN]
- Complete specifications already exist - no guessing
- Implement exactly as specified in research
- Validate against research document requirements

**TESTING DISCIPLINE:**
- Write unit tests for each module
- Write integration tests for each phase
- Test before moving to next component
- Keep test coverage high
- Performance benchmarks for critical paths

███████████████████████████████████████████████████████████████████████
██ PHASE 0: PROJECT FOUNDATION & BUILD INFRASTRUCTURE                 ██
███████████████████████████████████████████████████████████████████████

Goal: Clean slate with proper build system and directory structure.

═══════════════════════════════════════════════════════════════════════
0.1: Build System Setup
═══════════════════════════════════════════════════════════════════════

[ ] 0.1.1: Clean CMakeLists.txt
    Location: ./CMakeLists.txt
    Action: Remove all old source file references
    - Remove references to archived source files
    - Keep LLVM configuration (LLVM 20.1.2)
    - Set C++17 standard minimum
    - Configure proper warning flags (-Wall -Wextra)
    Test: CMake configures without errors
    
[ ] 0.1.2: Create directory structure
    Action: Create all necessary directories
    mkdir -p src/frontend/lexer
    mkdir -p src/frontend/parser
    mkdir -p src/frontend/ast
    mkdir -p src/frontend/sema
    mkdir -p src/frontend/module
    mkdir -p src/backend/ir
    mkdir -p src/backend/codegen
    mkdir -p src/runtime/gc
    mkdir -p src/runtime/allocators
    mkdir -p src/runtime/io
    mkdir -p src/runtime/process
    mkdir -p include/frontend
    mkdir -p include/backend
    mkdir -p include/runtime
    mkdir -p tests/unit
    mkdir -p tests/integration
    mkdir -p tests/benchmarks
    Test: All directories exist
    
[ ] 0.1.3: Create build scripts
    File: build.sh
    - Clean build script (rm -rf build && mkdir build && cd build && cmake .. && make)
    - Colorized output
    - Error checking
    Make executable: chmod +x build.sh
    
    File: test.sh
    - Run all unit tests
    - Run all integration tests
    - Report pass/fail summary
    Make executable: chmod +x test.sh
    
    File: clean.sh
    - Remove build artifacts
    - Keep source intact
    Make executable: chmod +x clean.sh
    
    Test: Each script runs without error
    
[ ] 0.1.4: Create .gitignore
    Action: Update .gitignore for clean repo
    Ignore: build/, *.o, *.so, *.a, *.ll, *.bc, core, a.out
    Keep: src/, include/, tests/, docs/, lib/
    Test: git status shows only tracked files

═══════════════════════════════════════════════════════════════════════
0.2: Testing Infrastructure
═══════════════════════════════════════════════════════════════════════

[ ] 0.2.1: Choose unit test framework
    Decision: Use simple assert-based tests or integrate Catch2/Google Test
    Recommendation: Start with assert-based, upgrade later if needed
    
[ ] 0.2.2: Create test runner
    File: tests/test_runner.cpp
    - Discovers and runs all test files
    - Reports results
    - Returns non-zero exit code on failure
    
[ ] 0.2.3: Create test helpers
    File: tests/test_helpers.h
    - Assertion macros
    - Test setup/teardown utilities
    - Common test data
    
[ ] 0.2.4: Integration test harness
    File: tests/integration/runner.sh
    - Compiles .aria files
    - Runs compiled executables
    - Checks output against expected
    - Reports pass/fail
    Make executable
    
[ ] 0.2.5: Add tests to CMakeLists.txt
    - Add test targets
    - Link with testing framework
    - Enable CTest integration
    Test: ctest runs (even if no tests yet)

COMMIT CHECKPOINT: Phase 0 - Build infrastructure ready
Tag: v0.1.0-phase0-complete
Merge to main: YES (infrastructure is foundation)

███████████████████████████████████████████████████████████████████████
██ PHASE 1: LEXER - TOKENIZATION FOUNDATION                           ██
███████████████████████████████████████████████████████████████████████

Goal: Convert source text into stream of tokens.
Reference: [research_012] for type tokens, aria_specs.txt for all tokens

═══════════════════════════════════════════════════════════════════════
1.1: Token Definitions (Must be complete before lexer implementation)
═══════════════════════════════════════════════════════════════════════

[ ] 1.1.1: Create token type enum
    File: include/frontend/token.h
    Content: Complete TokenType enum with ALL tokens
    Categories:
    - Keywords (75+ tokens)
    - Operators (50+ tokens)
    - Literals (7 types)
    - Punctuation (12+ tokens)
    - Special (EOF, ERROR, etc.)
    
    Keyword tokens to include:
    // Memory qualifiers
    TOKEN_KW_WILD, TOKEN_KW_WILDX, TOKEN_KW_STACK, TOKEN_KW_GC,
    
    // Control flow
    TOKEN_KW_IF, TOKEN_KW_ELSE, TOKEN_KW_WHILE, TOKEN_KW_FOR,
    TOKEN_KW_LOOP, TOKEN_KW_TILL, TOKEN_KW_WHEN, TOKEN_KW_THEN, TOKEN_KW_END,
    TOKEN_KW_PICK, TOKEN_KW_BREAK, TOKEN_KW_CONTINUE, TOKEN_KW_RETURN,
    TOKEN_KW_DEFER, TOKEN_KW_FALL,
    
    // Declarations
    TOKEN_KW_FUNC, TOKEN_KW_TYPE, TOKEN_KW_STRUCT, TOKEN_KW_USE,
    TOKEN_KW_MOD, TOKEN_KW_PUB, TOKEN_KW_EXTERN, TOKEN_KW_CONST,
    
    // Types - Integers
    TOKEN_KW_INT1, TOKEN_KW_INT2, TOKEN_KW_INT4, TOKEN_KW_INT8,
    TOKEN_KW_INT16, TOKEN_KW_INT32, TOKEN_KW_INT64, TOKEN_KW_INT128,
    TOKEN_KW_INT256, TOKEN_KW_INT512,
    
    // Types - Unsigned Integers
    TOKEN_KW_UINT8, TOKEN_KW_UINT16, TOKEN_KW_UINT32, TOKEN_KW_UINT64,
    TOKEN_KW_UINT128, TOKEN_KW_UINT256, TOKEN_KW_UINT512,
    
    // Types - TBB (Twisted Balanced Binary)
    TOKEN_KW_TBB8, TOKEN_KW_TBB16, TOKEN_KW_TBB32, TOKEN_KW_TBB64,
    
    // Types - Floating Point
    TOKEN_KW_FLT32, TOKEN_KW_FLT64, TOKEN_KW_FLT128, TOKEN_KW_FLT256, TOKEN_KW_FLT512,
    
    // Types - Special
    TOKEN_KW_BOOL, TOKEN_KW_STRING, TOKEN_KW_DYN, TOKEN_KW_OBJ,
    TOKEN_KW_RESULT, TOKEN_KW_VOID,
    
    // Types - Balanced Ternary/Nonary
    TOKEN_KW_TRIT, TOKEN_KW_TRYTE, TOKEN_KW_NIT, TOKEN_KW_NYTE,
    
    // Types - Vectors/Math
    TOKEN_KW_VEC2, TOKEN_KW_VEC3, TOKEN_KW_VEC9,
    TOKEN_KW_MATRIX, TOKEN_KW_TENSOR,
    
    // Special keywords
    TOKEN_KW_ASYNC, TOKEN_KW_IS, TOKEN_KW_NULL,
    
    // Literals
    TOKEN_KW_TRUE, TOKEN_KW_FALSE, TOKEN_KW_ERR,
    
    Operator tokens:
    // Arithmetic
    TOKEN_PLUS, TOKEN_MINUS, TOKEN_STAR, TOKEN_SLASH, TOKEN_PERCENT,
    TOKEN_PLUS_PLUS, TOKEN_MINUS_MINUS,
    
    // Assignment
    TOKEN_EQUAL, TOKEN_PLUS_EQUAL, TOKEN_MINUS_EQUAL,
    TOKEN_STAR_EQUAL, TOKEN_SLASH_EQUAL, TOKEN_PERCENT_EQUAL,
    
    // Comparison
    TOKEN_EQUAL_EQUAL, TOKEN_BANG_EQUAL,
    TOKEN_LESS, TOKEN_LESS_EQUAL,
    TOKEN_GREATER, TOKEN_GREATER_EQUAL,
    TOKEN_SPACESHIP, // <=>
    
    // Logical
    TOKEN_AND_AND, TOKEN_OR_OR, TOKEN_BANG,
    
    // Bitwise
    TOKEN_AMPERSAND, TOKEN_PIPE, TOKEN_CARET, TOKEN_TILDE,
    TOKEN_SHIFT_LEFT, TOKEN_SHIFT_RIGHT,
    
    // Special operators
    TOKEN_AT, TOKEN_DOLLAR, TOKEN_HASH,
    TOKEN_ARROW, TOKEN_SAFE_NAV, TOKEN_NULL_COALESCE,
    TOKEN_QUESTION, TOKEN_PIPE_RIGHT, TOKEN_PIPE_LEFT,
    TOKEN_DOT_DOT, TOKEN_DOT_DOT_DOT,
    
    // Punctuation
    TOKEN_DOT, TOKEN_COMMA, TOKEN_COLON, TOKEN_SEMICOLON,
    TOKEN_LEFT_PAREN, TOKEN_RIGHT_PAREN,
    TOKEN_LEFT_BRACE, TOKEN_RIGHT_BRACE,
    TOKEN_LEFT_BRACKET, TOKEN_RIGHT_BRACKET,
    TOKEN_BACKTICK,
    
    Test: Enum compiles, all values unique
    
[ ] 1.1.2: Create Token struct
    File: include/frontend/token.h (same file)
    Content:
    ```cpp
    struct Token {
        TokenType type;
        std::string lexeme;      // Raw text from source
        int line;                // Line number (1-indexed)
        int column;              // Column number (1-indexed)
        
        // Value storage (union for efficiency)
        union {
            int64_t int_value;
            double float_value;
            bool bool_value;
        } value;
        
        std::string string_value; // For strings (separate due to complex type)
        
        // Constructor
        Token(TokenType t, std::string lex, int ln, int col);
        
        // Helpers
        bool isKeyword() const;
        bool isOperator() const;
        bool isLiteral() const;
        std::string toString() const; // For debugging
    };
    ```
    Test: Token creation, field access works
    
[ ] 1.1.3: Implement Token methods
    File: src/frontend/lexer/token.cpp
    Implement: Constructor, isKeyword(), isOperator(), isLiteral(), toString()
    Test: All methods work correctly

═══════════════════════════════════════════════════════════════════════
1.2: Lexer Core Implementation
═══════════════════════════════════════════════════════════════════════
Reference: [research_012], [research_002], [research_003]

[ ] 1.2.1: Create Lexer class definition
    File: include/frontend/lexer/lexer.h
    Content:
    ```cpp
    class Lexer {
    private:
        std::string source;       // Source code
        size_t current;           // Current position
        size_t start;             // Start of current token
        int line;                 // Current line (1-indexed)
        int column;               // Current column (1-indexed)
        
        std::vector<std::string> errors; // Error messages
        
        // Helpers
        char advance();
        char peek() const;
        char peekNext() const;
        bool isAtEnd() const;
        bool match(char expected);
        
        void skipWhitespace();
        void skipLineComment();
        void skipBlockComment();
        
        Token makeToken(TokenType type);
        Token errorToken(const std::string& message);
        
        Token scanNumber();
        Token scanIdentifier();
        Token scanString();
        Token scanTemplateLiteral();
        Token scanOperator();
        
        TokenType identifierType(); // Check if identifier is keyword
        TokenType checkKeyword(int start, int length, const char* rest, TokenType type);
        
    public:
        explicit Lexer(const std::string& src);
        Token nextToken();
        const std::vector<std::string>& getErrors() const;
        bool hasErrors() const;
    };
    ```
    Test: Class compiles
    
[ ] 1.2.2: Implement Lexer constructor and basic helpers
    File: src/frontend/lexer/lexer.cpp
    Implement:
    - Lexer(const std::string& src) - Initialize members
    - advance() - Move to next char, update line/column
    - peek() - Look at current char without consuming
    - peekNext() - Look ahead 2 chars
    - isAtEnd() - Check if at EOF
    - match(char) - Consume if matches
    - makeToken() - Create token with current position
    - errorToken() - Create error token
    
    Test: Can create lexer, advance through string
    
[ ] 1.2.3: Implement whitespace handling
    File: src/frontend/lexer/lexer.cpp
    Implement:
    - skipWhitespace() - Skip spaces, tabs, newlines (track line numbers!)
    - skipLineComment() - Skip // comments
    - skipBlockComment() - Skip /* */ comments (handle nesting!)
    
    Test cases:
    - Mixed whitespace
    - Line comments
    - Block comments
    - Nested block comments
    - Comments at EOF
    
[ ] 1.2.4: Implement number lexing
    File: src/frontend/lexer/lexer.cpp
    Function: Token scanNumber()
    Handle:
    - Decimal: 123, 456
    - Hex: 0xFF, 0xDEADBEEF
    - Binary: 0b1010, 0b11110000
    - Octal: 0o755, 0o644
    - Floats: 3.14, 1.5e10, 0x1.2p3
    - Underscores: 1_000_000, 0xFF_FF
    
    Store value in token.value union
    
    Test cases:
    - All number formats
    - Edge cases (0, -1, MAX_INT)
    - Invalid numbers (error recovery)
    
[ ] 1.2.5: Implement identifier and keyword lexing
    File: src/frontend/lexer/lexer.cpp
    Function: Token scanIdentifier()
    - Lex identifier: [a-zA-Z_][a-zA-Z0-9_]*
    - Call identifierType() to check if keyword
    - Return TOKEN_IDENTIFIER or keyword token
    
    Function: TokenType identifierType()
    - Use trie or hash map for keyword lookup
    - Implement checkKeyword() helper for trie traversal
    
    Test cases:
    - All keywords recognized
    - Identifiers don't match keywords
    - Edge cases (i, x, _var, var123)
    
[ ] 1.2.6: Implement string lexing
    File: src/frontend/lexer/lexer.cpp
    Function: Token scanString()
    - Handle "double quoted" strings
    - Handle 'single quoted' strings
    - Escape sequences: \n, \t, \r, \\, \", \'
    - Unicode escapes: \u{1F600}
    - Detect unterminated strings (error)
    - Store content in token.string_value
    
    Test cases:
    - Basic strings
    - All escape sequences
    - Unicode
    - Unterminated strings
    - Empty strings
    
[ ] 1.2.7: Implement template literal lexing
    File: src/frontend/lexer/lexer.cpp
    Function: Token scanTemplateLiteral()
    Strategy: Lex `backtick strings` with &{interpolation}
    Approach: Return multiple tokens:
    - TOKEN_BACKTICK_START: `
    - TOKEN_TEMPLATE_TEXT: text parts
    - TOKEN_INTERP_START: &{
    - (nested expression tokens)
    - TOKEN_INTERP_END: }
    - TOKEN_BACKTICK_END: `
    
    Handle:
    - Nested expressions
    - Escape sequences including \`
    - Multiple interpolations
    
    Test cases:
    - Simple templates
    - Interpolations
    - Nested expressions
    - Escape sequences
    
[ ] 1.2.8: Implement operator lexing
    File: src/frontend/lexer/lexer.cpp
    Function: Token scanOperator()
    Strategy: Maximal munch (longest match wins)
    
    Multi-char operators to handle:
    - <=>, ?., ??, |>, <|, .., ..., ->
    - ++, --, ==, !=, <=, >=, &&, ||
    - <<, >>, +=, -=, *=, /=, %=
    
    Use cascading if/match:
    if (peek() == '=') {
        if (peekNext() == '=') { /* ... */ }
    }
    
    Test cases:
    - All operators
    - Disambiguation (< vs <= vs <=> vs <|)
    
[ ] 1.2.9: Implement main tokenization loop
    File: src/frontend/lexer/lexer.cpp
    Function: Token nextToken()
    
    Algorithm:
    1. Skip whitespace and comments
    2. Save start position
    3. If at end, return TOKEN_EOF
    4. Read first char, dispatch:
       - Digit or . followed by digit -> scanNumber()
       - Letter or _ -> scanIdentifier()
       - " or ' -> scanString()
       - ` -> scanTemplateLiteral()
       - Operator char -> scanOperator()
       - Punctuation -> single-char token
       - Unknown -> errorToken()
    5. Return token
    
    Test: Can lex complete programs
    
[ ] 1.2.10: Implement error recovery
    File: src/frontend/lexer/lexer.cpp
    Functions:
    - errorToken() - Create TOKEN_ERROR
    - Store error message
    - Continue lexing (don't crash)
    
    Strategy for errors:
    - Unknown character: Skip, report error, continue
    - Unterminated string: Report error, return partial string
    - Invalid number: Report error, return 0
    
    Test: Lexer finds multiple errors in one pass

═══════════════════════════════════════════════════════════════════════
1.3: Lexer Testing
═══════════════════════════════════════════════════════════════════════

[ ] 1.3.1: Unit test - Numbers
    File: tests/unit/test_lexer_numbers.cpp
    Test cases:
    - Decimal integers: 0, 123, 999999
    - Hex: 0x0, 0xFF, 0xDEADBEEF
    - Binary: 0b0, 0b1, 0b11111111
    - Octal: 0o0, 0o7, 0o755
    - Floats: 0.0, 3.14, 1e10, 1.5e-10
    - Underscores: 1_000_000
    - Invalid: 0x, 0b, 1.2.3 (expect errors)
    Run: ./test.sh unit_lexer_numbers
    
[ ] 1.3.2: Unit test - Strings
    File: tests/unit/test_lexer_strings.cpp
    Test cases:
    - Basic: "hello", 'world'
    - Empty: "", ''
    - Escapes: "line\nbreak", "tab\there"
    - Unicode: "\u{1F600}"
    - Unterminated: "hello (expect error)
    Run: ./test.sh unit_lexer_strings
    
[ ] 1.3.3: Unit test - Template literals
    File: tests/unit/test_lexer_templates.cpp
    Test cases:
    - Simple: `hello`
    - Interpolation: `value: &{x}`
    - Multiple: `&{a} + &{b} = &{a+b}`
    - Nested: `&{x + (y * 2)}`
    - Escape: `backtick: \``
    Run: ./test.sh unit_lexer_templates
    
[ ] 1.3.4: Unit test - Operators
    File: tests/unit/test_lexer_operators.cpp
    Test all operators individually
    Test disambiguation: < vs <= vs <=> vs <|
    Run: ./test.sh unit_lexer_operators
    
[ ] 1.3.5: Unit test - Keywords
    File: tests/unit/test_lexer_keywords.cpp
    Test all 75+ keywords recognized
    Test keywords vs identifiers (while vs while123)
    Run: ./test.sh unit_lexer_keywords
    
[ ] 1.3.6: Integration test - Complete file
    File: tests/integration/test_lexer_full.cpp
    Test: Lex complete valid Aria program
    Verify: All tokens correct
    Test error cases: Invalid syntax caught
    Run: ./test.sh integration_lexer_full
    
[ ] 1.3.7: Benchmark - Lexer performance
    File: tests/benchmarks/bench_lexer.cpp
    Test: Large file (10,000+ lines)
    Measure: Tokens per second
    Target: > 100,000 tokens/sec
    Run: ./test.sh bench_lexer

COMMIT CHECKPOINT: Phase 1 - Lexer complete
Tag: v0.1.0-phase1-lexer-complete
All tests pass
Merge to main: YES (lexer is foundation)

(To be continued in next response due to length...)

